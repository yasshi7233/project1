{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline Model - Text Features Only\n",
        "\n",
        "This notebook implements a baseline model using only text features from catalog_content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load preprocessed data\n",
        "train_df = pd.read_csv('../dataset/train_preprocessed.csv')\n",
        "print(f\"Training data shape: {train_df.shape}\")\n",
        "\n",
        "# Check if preprocessing was done\n",
        "if 'cleaned_text' not in train_df.columns:\n",
        "    print(\"Preprocessing not found, running preprocessing...\")\n",
        "    \n",
        "    def extract_ipq(text):\n",
        "        patterns = [\n",
        "            r'pack of (\\d+)',\n",
        "            r'(\\d+) pack',\n",
        "            r'quantity[\\s:]+(\\d+)',\n",
        "            r'ipq[\\s:]+(\\d+)',\n",
        "            r'(\\d+)\\s*x\\s*(\\d+)',\n",
        "        ]\n",
        "        \n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, text.lower())\n",
        "            if match:\n",
        "                if len(match.groups()) == 2:\n",
        "                    return int(match.group(1)) * int(match.group(2))\n",
        "                else:\n",
        "                    return int(match.group(1))\n",
        "        return 1\n",
        "\n",
        "    def clean_text(text):\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    train_df['cleaned_text'] = train_df['catalog_content'].apply(clean_text)\n",
        "    train_df['ipq'] = train_df['catalog_content'].apply(extract_ipq)\n",
        "    train_df['text_length'] = train_df['cleaned_text'].str.len()\n",
        "    train_df['word_count'] = train_df['cleaned_text'].str.split().str.len()\n",
        "\n",
        "print(\"Data ready for modeling!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features\n",
        "print(\"Preparing features...\")\n",
        "\n",
        "# TF-IDF features\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    ngram_range=(1, 2),\n",
        "    stop_words='english',\n",
        "    min_df=5,\n",
        "    max_df=0.95\n",
        ")\n",
        "\n",
        "tfidf_features = tfidf.fit_transform(train_df['cleaned_text'])\n",
        "print(f\"TF-IDF features shape: {tfidf_features.shape}\")\n",
        "\n",
        "# Numerical features\n",
        "numerical_features = ['ipq', 'text_length', 'word_count']\n",
        "X_numerical = train_df[numerical_features].values\n",
        "\n",
        "# Combine features\n",
        "from scipy.sparse import hstack\n",
        "X_combined = hstack([tfidf_features, X_numerical])\n",
        "print(f\"Combined features shape: {X_combined.shape}\")\n",
        "\n",
        "# Target variable\n",
        "y = train_df['price'].values\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "\n",
        "# Split data\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_combined, y, test_size=0.2, random_state=42\n",
        ")\n",
        "print(f\"Train set: {X_train.shape}, Validation set: {X_val.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SMAPE metric function\n",
        "def smape(y_true, y_pred):\n",
        "    \"\"\"Symmetric Mean Absolute Percentage Error\"\"\"\n",
        "    return np.mean(np.abs(y_true - y_pred) / ((np.abs(y_true) + np.abs(y_pred)) / 2)) * 100\n",
        "\n",
        "# Train LightGBM model\n",
        "print(\"Training LightGBM model...\")\n",
        "lgb_model = lgb.LGBMRegressor(\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=8,\n",
        "    num_leaves=31,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "lgb_model.fit(X_train, y_train)\n",
        "lgb_pred = lgb_model.predict(X_val)\n",
        "\n",
        "lgb_mae = mean_absolute_error(y_val, lgb_pred)\n",
        "lgb_smape = smape(y_val, lgb_pred)\n",
        "\n",
        "print(f\"LightGBM - MAE: {lgb_mae:.2f}, SMAPE: {lgb_smape:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train XGBoost model\n",
        "print(\"Training XGBoost model...\")\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=8,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    verbosity=0\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_train, y_train)\n",
        "xgb_pred = xgb_model.predict(X_val)\n",
        "\n",
        "xgb_mae = mean_absolute_error(y_val, xgb_pred)\n",
        "xgb_smape = smape(y_val, xgb_pred)\n",
        "\n",
        "print(f\"XGBoost - MAE: {xgb_mae:.2f}, SMAPE: {xgb_smape:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensemble model\n",
        "print(\"Training ensemble model...\")\n",
        "ensemble_pred = (lgb_pred + xgb_pred) / 2\n",
        "\n",
        "ensemble_mae = mean_absolute_error(y_val, ensemble_pred)\n",
        "ensemble_smape = smape(y_val, ensemble_pred)\n",
        "\n",
        "print(f\"Ensemble - MAE: {ensemble_mae:.2f}, SMAPE: {ensemble_smape:.2f}%\")\n",
        "\n",
        "# Results summary\n",
        "print(\"\\nModel Performance Summary:\")\n",
        "print(f\"LightGBM: MAE={lgb_mae:.2f}, SMAPE={lgb_smape:.2f}%\")\n",
        "print(f\"XGBoost: MAE={xgb_mae:.2f}, SMAPE={xgb_smape:.2f}%\")\n",
        "print(f\"Ensemble: MAE={ensemble_mae:.2f}, SMAPE={ensemble_smape:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save models and preprocessing objects\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Create models directory\n",
        "os.makedirs('../models', exist_ok=True)\n",
        "\n",
        "# Save models\n",
        "joblib.dump(lgb_model, '../models/lgb_baseline.pkl')\n",
        "joblib.dump(xgb_model, '../models/xgb_baseline.pkl')\n",
        "joblib.dump(tfidf, '../models/tfidf_vectorizer.pkl')\n",
        "\n",
        "print(\"Models and preprocessing objects saved!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
